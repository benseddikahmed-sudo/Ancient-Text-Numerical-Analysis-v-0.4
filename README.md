# Ancient-Text-Numerical-Analysis-v-0.4

[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![OSF DOI](https://img.shields.io/badge/OSF-10.17605%2FOSF.IO%2FGXQH6-blue.svg)](https://doi.org/10.17605/OSF.IO/GXQH6)
[![Zenodo DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17643428.svg)](https://doi.org/10.5281/zenodo.17643428)
[![ORCID](https://img.shields.io/badge/ORCID-0009--0005--6308--8171-green.svg)](https://orcid.org/0009-0005-6308-8171)

> **A rigorous, reproducible computational framework for detecting and validating numerical patterns in ancient texts**

**Publication:** Submitted to *Digital Scholarship in the Humanities* (DSH) â€” November 2025  
**Author:** Ahmed Benseddik ([ORCID: 0009-0005-6308-8171](https://orcid.org/0009-0005-6308-8171))  
**Version:** v0.4  
**Date:** November 2025  
**Pre-registration:** [OSF DOI: 10.17605/OSF.IO/GXQH6](https://doi.org/10.17605/OSF.IO/GXQH6)  
**Code Archive:** [Zenodo DOI: 10.5281/zenodo.17643428](https://doi.org/10.5281/zenodo.17643428)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Methodology](#-methodology)
- [Case Study: Genesis](#-case-study-genesis)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Repository Structure](#-repository-structure)
- [Usage Examples](#-usage-examples)
- [Analysis Pipeline](#-analysis-pipeline)
- [Interpreting Results](#-interpreting-results)
- [Testing](#-testing)
- [Documentation](#-documentation)
- [Reproducibility](#-reproducibility)
- [Citation](#-citation)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸ”¬ Overview

This repository contains the complete implementation of a **three-phase computational framework** for detecting and validating numerical patterns in ancient texts, with comprehensive case studies of Genesis (Sefer Bereshit) and support for multiple cultural numerical systems. The framework is designed for digital humanities scholarship with rigorous methodological standards.

### The Problem

Analysis of numerical patterns in sacred texts (gematria, isopsephy, abjad) has long been controversial, plagued by:
- âŒ Selective reporting of positive results only
- âŒ Post-hoc rationalization without pre-registration
- âŒ Lack of independent validation
- âŒ Insufficient statistical rigor and multiple testing corrections
- âŒ Absence of expert consensus mechanisms

### Our Solution

A systematic, **pre-registered** framework ([OSF: 10.17605/OSF.IO/GXQH6](https://doi.org/10.17605/OSF.IO/GXQH6)) integrating three independent validation phases:

1. **Unsupervised Discovery** â€” Pattern detection using pre-registered structural markers
2. **Multi-Method Statistical Validation** â€” Convergent evidence from frequentist, Bayesian, and bootstrap methods
3. **Structured Expert Consensus** â€” Modified Delphi protocol with interdisciplinary panel

### Innovation

âœ¨ **First integrated framework** combining:
- Computational pattern discovery
- Rigorous statistical validation (frequentist + Bayesian)
- Qualitative expert assessment
- Diachronic manuscript validation
- Complete pre-registration and reproducibility

This addresses a critical gap in digital humanities: how to rigorously validate computational claims about ancient texts while preventing confirmation bias and p-hacking.

---

## âœ¨ Key Features

### ğŸ”¬ Methodological Rigor

âœ… **Pre-registered hypotheses** ([OSF: 10.17605/OSF.IO/GXQH6](https://doi.org/10.17605/OSF.IO/GXQH6)) â€” All markers and parameters registered *before* statistical testing  
âœ… **Discovery-validation separation** â€” Strict temporal separation prevents circular reasoning  
âœ… **Multiple testing corrections** â€” Bonferroni, Å idÃ¡k, Benjamini-Hochberg FDR at q=0.05  
âœ… **Effect sizes reported** â€” Cohen's d, h with 95% confidence intervals  
âœ… **Power analysis** â€” Ensures adequate sample size (target power â‰¥ 0.80)  
âœ… **Formal mathematical proofs** â€” 7 theorems with computational verification  

### ğŸ“Š Statistical Methods

**Frequentist Validation:**
- Permutation tests: 10,000-50,000 iterations with exact p-values
- Binomial tests: Exact confidence intervals (Wilson score method)
- Multiple testing corrections: Bonferroni, Å idÃ¡k, Benjamini-Hochberg FDR
- Effect sizes: Cohen's h, Cohen's d, standardized differences
- Bootstrap CI: Percentile and BCa methods (10,000 resamples)
- Power analysis: Sample size adequacy assessment (target power â‰¥ 0.80)

**Bayesian Validation:**
- Hierarchical models: Beta-Binomial conjugate priors
- MCMC sampling: PyMC with 4 chains, 5000+ draws, Gelman-Rubin diagnostics
- Convergence diagnostics: RÌ‚, effective sample size, trace plots
- Model comparison: WAIC, LOO-CV, Bayes Factors (BF)
- Posterior predictive checks: Distribution validation
- HDI intervals: Highest Density Intervals (95% credible intervals)

**Expert Validation:**
- Modified Delphi protocol (3 rounds)
- Interdisciplinary panel: 12 experts (4 philologists, 3 statisticians, 3 historians, 2 textual critics)
- Structured scoring: 0-10 scale across 4 criteria
- Consensus threshold: Mean â‰¥7.0 with SDâ‰¤1.5

### ğŸŒ Cross-Cultural Scope

**Multiple numerical systems supported:**
- **Hebrew Gematria:**
  - Standard (Mispar Hechrachi): Traditional Hebrew letter values
  - Atbash (letter reversal): ×â†”×ª, ×‘â†”×©, etc.
  - Albam (letter substitution): ×â†”×œ, ×‘â†”×, etc.
- **Greek Isopsephy:** Classical Greek numerical values (Î±=1, Î²=2, ..., Ï‰=800)
- **Arabic Abjad:** Traditional Arabic numerals (Ø£=1, Ø¨=2, Ø¬=3, etc.)
- **Cross-cultural correlation:** Statistical comparison across systems

### ğŸ”„ Complete Reproducibility

âœ… **Complete environment capture:** Python version, dependencies, system info  
âœ… **Git commit tracking:** Version control integration with tagged releases  
âœ… **Deterministic seeds:** All random processes reproducible (seed=42)  
âœ… **Comprehensive logging:** File + console outputs with timestamps  
âœ… **Metadata tracking:** Every analysis run documented with provenance  
âœ… **Pre-registration:** OSF registry for markers and parameters (locked record)  
âœ… **Code verification:** Independent R implementation validates Python results  

### ğŸ“œ Diachronic Validation

- **Manuscript stability** across 1,100 years of transmission
- **Qumran Dead Sea Scrolls** (ca. 100 BCE) â†’ **Leningrad Codex** (1008 CE)
- **Pattern preservation:** 91-100% stability across textual witnesses
- **Robustness demonstration:** Patterns survive scribal errors and textual variants

### ğŸ¨ Visualization & Reporting

ğŸ“Š **Publication-quality figures:** 300 DPI, vector formats (SVG, PDF)  
ğŸ“ˆ **Distribution plots:** Histograms with density curves, Q-Q plots  
ğŸ¨ **Bayesian diagnostics:** Trace plots, posterior distributions, forest plots  
ğŸ” **Sensitivity analysis:** Robustness visualizations across parameter space  
ğŸŒ **Cross-cultural heatmaps:** Correlation matrices for multi-system analysis  
ğŸ“‰ **Effect size plots:** Forest plots with confidence intervals  

### ğŸ”¬ Ethical Framework

ğŸ”¬ **Methodological transparency:** All assumptions documented and justified  
ğŸŒ **Cultural sensitivity:** Guidelines for respectful interpretation of religious texts  
âš ï¸ **Interpretation caveats:** Limitations clearly stated in all outputs  
ğŸ“ **Acknowledgment of uncertainty:** Probabilistic statements only, no deterministic claims  
ğŸ¤ **Community engagement:** Open to scholarly feedback and collaborative development  

---

## ğŸ§¬ Methodology

### Three-Phase Framework

```mermaid
graph TD
    A[Ancient Text] --> B[Phase 1: Discovery]
    B --> C[Frequency Analysis]
    B --> D[Gematria Calculation]
    B --> E[Positional Clustering]
    C --> F[Phase 2: Statistical Validation]
    D --> F
    E --> F
    F --> G[Permutation Tests]
    F --> H[Bayesian Analysis]
    F --> I[Bootstrap CI]
    F --> J[FDR Correction]
    G --> K[Phase 3: Expert Consensus]
    H --> K
    I --> K
    J --> K
    K --> L[Diachronic Validation]
    L --> M[Validated Patterns]
    
    style A fill:#e1f5ff
    style M fill:#d4edda
    style F fill:#fff3cd
    style K fill:#f8d7da
```

The framework combines:
- **Unsupervised Discovery** â€” Pattern detection via frequency analysis, gematria, and permutation scans
- **Statistical Validation** â€” Multiple tests (permutation, Bayesian, bootstrap) with FDR corrections
- **Expert Consensus** â€” Structured Delphi protocol with interdisciplinary panel

---

### Phase 1: Unsupervised Discovery

**Goal:** Detect pattern candidates *without* hypothesis testing to prevent data mining

**Input:** 
- Text corpus T (e.g., Genesis in Hebrew)
- Pre-registered markers M (structural divisions, chapter boundaries)

**Methods:**
1. **Frequency analysis** â€” Lexical distribution across text
2. **Gematria calculation** â€” Multiple cultural numerical systems
3. **Co-occurrence detection** â€” Term proximity analysis
4. **Positional clustering** â€” Association with structural markers

**Output:** Candidate patterns exceeding k=2 standard deviations from null expectation

**Critical principle:** No p-values computed in discovery phase. All candidates subjected to independent validation in Phase 2.

---

### Phase 2: Statistical Validation

**Goal:** Multi-method convergent validation with rigorous error control

**Validation Requirements (ALL must be met):**

âœ… **Permutation p-value < 0.01** (after FDR correction)  
âœ… **Bayes Factor > 10** (strong evidence for Hâ‚)  
âœ… **Large effect size** (Cohen's d > 0.8)  
âœ… **Significant after FDR** (q < 0.05)  

**Statistical Pipeline:**

1. **Permutation tests:** 10,000-50,000 iterations, exact p-values
   - Null model: Random lexical permutation preserving frequencies
   - One-tailed test: P(X â‰¥ observed | Hâ‚€)
   - Effect size: Cohen's d with 95% bootstrap CI

2. **Bayesian analysis:** Bayes Factors with Beta priors (BF > 10 threshold)
   - Model comparison: Hâ‚€ (random) vs. Hâ‚ (structured)
   - Hierarchical Beta-Binomial models
   - Prior sensitivity analysis

3. **Bootstrap CI:** 10,000 resamples, 95% confidence intervals
   - BCa (bias-corrected and accelerated) method
   - Percentile method for comparison

4. **FDR correction:** Benjamini-Hochberg at q=0.05
   - Controls expected proportion of false discoveries
   - More powerful than Bonferroni for multiple hypotheses

5. **Effect sizes:** Cohen's d, h with interpretation guidelines
   - Small (0.2), medium (0.5), large (0.8) effects
   - Standardized for cross-pattern comparison

6. **Power analysis:** Sample size adequacy (target power â‰¥ 0.80)
   - Post-hoc power calculation
   - Ensures sufficient sensitivity to detect effects

---

### Phase 3: Expert Consensus

**Goal:** Qualitative validation by domain experts using structured protocol

**Panel Composition:**
- 4 Hebrew philologists (biblical text specialists)
- 3 statisticians (quantitative methods experts)
- 3 historians (ancient Near East context)
- 2 textual critics (manuscript transmission specialists)

**Modified Delphi Protocol:**

**Round 1:** Blind assessment (no statistical results disclosed)
- Experts evaluate patterns based solely on textual/historical plausibility
- Individual scoring without group discussion

**Round 2:** Re-evaluation with statistical disclosure
- Statistical results (p-values, BF, effect sizes) now provided
- Experts revise initial assessments considering quantitative evidence

**Round 3:** Consensus discussion with facilitation
- Group discussion to resolve discrepancies
- Final consensus scoring

**Scoring Criteria (0-10 scale):**
1. **Historical plausibility** (0-3 points) â€” Does pattern align with known literary/theological traditions?
2. **Textual coherence** (0-3 points) â€” Is pattern internally consistent across text?
3. **Manuscript stability** (0-2 points) â€” Pattern preserved across textual witnesses?
4. **Statistical strength** (0-2 points) â€” Quantitative evidence compelling?

**Consensus Threshold:** Mean â‰¥ 7.0 with SD â‰¤ 1.5

---

### Combined Validation Criteria

A pattern is **validated** if and only if **ALL** criteria are met:

âœ… Permutation p-value < 0.01  
âœ… Bayes Factor > 10 (strong evidence)  
âœ… Expert consensus â‰¥ 7.0  
âœ… Diachronic stability â‰¥ 90%  

**Theorem (Combined Type-I Error Control):** Under the global null hypothesis, the framework controls family-wise error rate at Î± â‰¤ 0.05.

**Proof:** See [docs/mathematical_proofs.pdf](docs/mathematical_proofs.pdf) (Theorem 4, page 12).

---

## ğŸ“– Case Study: Genesis

We demonstrate the framework through comprehensive analysis of **Genesis (Sefer Bereshit)** from the Westminster Leningrad Codex.

### Validated Patterns

| Pattern | Hebrew | Value/Count | p-value | Bayes Factor | Expert Score | Stability |
|---------|--------|-------------|---------|--------------|--------------|-----------|
| **Toledot** | ×ª×•×œ×“×•×ª | 846 (gematria) | 0.007 | 18.7 | 8.2/10 | 96.7% |
| **Ha-Tebah** | ×”×ª×‘×” | 17 occurrences | 0.010 | 21.6 | 8.3/10 | 98.0% |
| **Sum 1260** | â€” | 3 instances | 0.012 | 14.3 | 7.9/10 | 100% |
| **Sum 1290** | â€” | 2 instances | 0.019 | 12.4 | 8.1/10 | 100% |
| **Sum 1335** | â€” | 2 instances | 0.015 | 14.9 | 7.5/10 | 100% |

*All patterns remain significant after FDR correction (q < 0.05)*

---

### Pattern 1: Toledot Formula (×ª×•×œ×“×•×ª)

**Gematria value:** 846  
**Structural role:** Marks 10 genealogical divisions in Genesis  
**Biblical context:** Gen 2:4, 5:1, 6:9, 10:1, 11:10, 11:27, 25:12, 25:19, 36:1, 37:2

**Validation:**
- **Permutation test:** p = 0.007 (highly significant)
- **Bayes Factor:** BF = 18.7 (strong evidence for Hâ‚)
- **Effect size:** Cohen's d = 2.84 (very large effect)
- **Expert consensus:** 8.2/10 (high agreement)
- **Manuscript stability:** 96.7% across Qumran, Aleppo, Leningrad

**Interpretation:** Well-known literary marker in biblical scholarship; gematria value 846 aligns with the ten structural divisions created by the Toledot formula, reinforcing its architectural significance in Genesis composition.

---

### Pattern 2: Ha-Tebah Lexeme (×”×ª×‘×”, "The Ark")

**Occurrences:** 17 times in Genesis  
**Context:** Specific to Noah narrative (Genesis 6-9)  
**Clustering:** At narrative transition markers

**Validation:**
- **Permutation test:** p = 0.010 (highly significant)
- **Bayes Factor:** BF = 21.6 (strong evidence)
- **Effect size:** Cohen's d = 4.19 (very large effect)
- **Expert consensus:** 8.3/10 (highest score)
- **Manuscript stability:** 98.0%

**Robustness checks:**
âœ… Pattern maintained within Noah narrative alone (p = 0.023)  
âœ… Specific to flood account (p = 0.18 when Noah chapters excluded, as expected)  
âœ… Clustering at structurally significant points (ark construction, animals entering, flood receding)

---

### Pattern 3-5: Intertextual Sums (1260, 1290, 1335)

**Biblical context:** Correspond to prophetic chronologies in Daniel 12:7, 12:11, 12:12 and Revelation 11:3, 12:6

**Occurrence counts:**
- Sum 1260: 3 instances in Genesis
- Sum 1290: 2 instances in Genesis
- Sum 1335: 2 instances in Genesis

**Validation:**
- **All Bayes Factors > 12** (strong evidence)
- **Expert consensus â‰¥ 7.5** across all three patterns
- **Manuscript stability:** 100% across all textual witnesses

**Significance:** Potential numerical intertextuality across biblical corpus, suggesting deliberate compositional links between Genesis and apocalyptic literature. However, experts noted interpretive caution required given small sample sizes.

---

### Sensitivity Analysis

**Robustness checks confirm pattern validity:**

âœ… **Alternative marker definitions:** Patterns robust across 3 different structural marker sets (p â‰¤ 0.02 in all)  
âœ… **Subsampling:** Ha-Tebah specific to Noah narrative (as expected; p = 0.18 when excluded)  
âœ… **Random seed variation:** P-values stable within Â±0.005 across 10 different random seeds  
âœ… **Manuscript variations:** 91-100% stability across Qumran, Aleppo, Leningrad codices  
âœ… **Window size variation:** Results consistent across different analytical window sizes  

---

## ğŸ› ï¸ Installation

### Requirements

- **Python 3.9 or higher**
- **Git** (for cloning repository)
- **(Optional)** LaTeX distribution for compiling mathematical proofs

### Quick Install

```bash
# Clone the repository
git clone https://github.com/benseddikahmed-sudo/Ancient-Text-Numerical-Analysis-v-0.4.git
cd Ancient-Text-Numerical-Analysis-v-0.4

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

**Core Requirements (Frequentist Analysis):**
```
numpy>=1.24.0      # Numerical computing
scipy>=1.10.0      # Scientific computing
pandas>=2.0.0      # Data manipulation
matplotlib>=3.7.0  # Plotting
seaborn>=0.12.0    # Statistical visualization
statsmodels>=0.14.0 # Statistical models
jupyter>=1.0.0     # Interactive notebooks
pytest>=7.0.0      # Testing framework
```

**Optional (Bayesian Analysis):**
```
pymc>=5.0.0        # Bayesian inference (optional)
arviz>=0.15.0      # Bayesian diagnostics (optional)
numba>=0.57.0      # JIT compilation (optional)
```

**Minimal install (frequentist-only):**
```bash
pip install numpy scipy pandas matplotlib seaborn statsmodels
```

---

## ğŸš€ Quick Start

### Verify Installation

```bash
# Run test suite
python -m pytest tests/ -v

# Run theorem demonstrations
python src/theorem_demonstrations.py

# Check environment
python -c "import ancient_text_dsh; print(ancient_text_dsh.__version__)"
```

**Expected output:** All tests should pass âœ…

---

### Basic Usage

```python
from ancient_text_dsh import AnalysisConfig, AncientTextAnalysisPipeline

# Configure analysis
config = AnalysisConfig(
    data_dir='data/',
    output_dir='results/',
    n_permutations=50000,
    n_bayesian_draws=5000,
    enable_bayesian=True,
    significance_level=0.01,
    fdr_level=0.05
)

# Run complete pipeline
pipeline = AncientTextAnalysisPipeline(config)
results = pipeline.run_complete_analysis()

# View validated patterns
print(f"Validated patterns: {len(results['validated_patterns'])}")
print(f"Mean Bayes Factor: {results['summary']['mean_bayes_factor']:.2f}")
print(f"FDR-adjusted significance: {results['summary']['fdr_threshold']:.4f}")
```

---

### Command-Line Interface

```bash
# Complete analysis with all features
python dsh_framework.py --data-dir ./data/genesis --output-dir ./results

# Fast analysis (no Bayesian, fewer permutations)
python dsh_framework.py --no-bayesian --n-permutations 10000

# High-quality analysis (publication-ready)
python dsh_framework.py --n-permutations 50000 --n-bayesian-draws 5000 --dpi 300
```

---

### Interactive Jupyter Notebooks

```bash
jupyter notebook notebooks/
```

**Start with:**
1. `01_exploratory_analysis.ipynb` â€” Data exploration and visualization
2. `02_permutation_tests.ipynb` â€” Statistical testing walkthrough
3. `03_bayesian_validation.ipynb` â€” Bayesian inference tutorial
4. `04_diachronic_checks.ipynb` â€” Manuscript comparison
5. `05_expert_panel_analysis.ipynb` â€” Delphi protocol implementation
6. `06_sensitivity_analyses.ipynb` â€” Robustness checks

---

## ğŸ“ Repository Structure

```
Ancient-Text-Numerical-Analysis-v-0.4/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ CHANGELOG.md                       # Version history
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                           # Package installation
â”‚
â”œâ”€â”€ data/                              # Source texts and annotations
â”‚   â”œâ”€â”€ genesis_leningrad.txt          # Westminster Leningrad Codex (Genesis)
â”‚   â”œâ”€â”€ structural_markers.json        # Pre-registered markers (43 total)
â”‚   â”œâ”€â”€ gematria_map.csv              # Hebrew letter â†’ numeric values
â”‚   â”œâ”€â”€ key_patterns.json              # 5 validated patterns with stats
â”‚   â”œâ”€â”€ analysis_config.json           # Pre-registered parameters
â”‚   â””â”€â”€ cultural_systems/              # Greek, Arabic mappings
â”‚       â”œâ”€â”€ greek_isopsephy.json
â”‚       â””â”€â”€ arabic_abjad.json
â”‚
â”œâ”€â”€ src/                               # Core analysis modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dsh_framework.py            # Main analysis script
â”‚   â”œâ”€â”€ permutation_tests.py           # Permutation test implementation
â”‚   â”œâ”€â”€ bayesian_analysis.py           # Bayes Factor calculations
â”‚   â”œâ”€â”€ gematria_calculator.py         # Multi-cultural gematria
â”‚   â”œâ”€â”€ diachronic_validation.py       # Manuscript comparison
â”‚   â”œâ”€â”€ expert_panel_analysis.py       # Delphi protocol scoring
â”‚   â”œâ”€â”€ fdr_correction.py              # Benjamini-Hochberg FDR
â”‚   â”œâ”€â”€ visualization_tools.py         # Plotting functions
â”‚   â””â”€â”€ theorem_demonstrations.py      # Mathematical proofs verification
â”‚
â”œâ”€â”€ notebooks/                         # Interactive analysis
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_permutation_tests.ipynb
â”‚   â”œâ”€â”€ 03_bayesian_validation.ipynb
â”‚   â”œâ”€â”€ 04_diachronic_checks.ipynb
â”‚   â”œâ”€â”€ 05_expert_panel_analysis.ipynb
â”‚   â””â”€â”€ 06_sensitivity_analyses.ipynb
â”‚
â”œâ”€â”€ results/                           # Analysis outputs
â”‚   â”œâ”€â”€ permutation_outputs.csv        # P-values for all patterns
â”‚   â”œâ”€â”€ bayes_factors.csv              # BF calculations
â”‚   â”œâ”€â”€ expert_scores.csv              # Delphi panel results
â”‚   â”œâ”€â”€ diachronic_stability.csv       # Manuscript preservation
â”‚   â”œâ”€â”€ theorem_verification_results.json
â”‚   â””â”€â”€ figures/                       # Publication-ready plots
â”‚       â”œâ”€â”€ theorem1_type1_control.png
â”‚       â”œâ”€â”€ theorem2_bf_consistency.png
â”‚       â”œâ”€â”€ theorem3_fdr_control.png
â”‚       â”œâ”€â”€ gematria_distribution.png
â”‚       â”œâ”€â”€ multiples_analysis.png
â”‚       â””â”€â”€ cross_cultural_heatmap.png
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ METHODOLOGY.md                 # Detailed methods
â”‚   â”œâ”€â”€ mathematical_proofs.pdf        # Complete proofs (25 pages)
â”‚   â”œâ”€â”€ mathematical_proofs.tex        # LaTeX source
â”‚   â”œâ”€â”€ proofs_summary.pdf             # 5-page summary
â”‚   â”œâ”€â”€ references.bib                 # BibTeX bibliography (40+ refs)
â”‚   â”œâ”€â”€ technical_slide.html           # Permutation visualization
â”‚   â”œâ”€â”€ infographic.html               # Framework visual summary
â”‚   â””â”€â”€ appendix_A_methodology.md      # Complete technical appendix
â”‚
â”œâ”€â”€ tests/                             # Unit and integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_permutation.py
â”‚   â”œâ”€â”€ test_bayesian.py
â”‚   â”œâ”€â”€ test_gematria.py
â”‚   â”œâ”€â”€ test_fdr.py
â”‚   â”œâ”€â”€ test_statistics.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â””â”€â”€ supplementary/                     # Additional materials
    â”œâ”€â”€ presentation_beamer.pdf        # Conference slides
    â”œâ”€â”€ poster_DSH2025.pdf             # Conference poster
    â””â”€â”€ media/                         # Presentation figures
```

---

## ğŸ’» Usage Examples

### Example 1: Permutation Test

```python
from src.permutation_tests import permutation_test
from src.bayesian_analysis import bayes_factor_binomial
import json

# Load configuration
with open('data/analysis_config.json', 'r') as f:
    config = json.load(f)

# Load markers
with open('data/structural_markers.json', 'r') as f:
    markers = json.load(f)

# Run permutation test for Ha-Tebah
result = permutation_test(
    corpus='data/genesis_leningrad.txt',
    target_term='×”×ª×‘×”',
    markers=markers['chapter_boundaries'],
    n_iterations=50000,
    seed=42
)

print(f"P-value: {result['p_value']:.5f}")
print(f"Observed count: {result['observed_count']}")
print(f"Expected (null): {result['null_mean']:.2f}")
print(f"Cohen's d: {result['cohens_d']:.2f}")

# Bayes Factor
bf = bayes_factor_binomial(
    observed_count=17,
    n_markers=43,
    corpus_length=20614,
    total_occurrences=17,
    alpha_prior=5.0,
    beta_prior=2.0
)

print(f"Bayes Factor: {bf:.1f}")
```

**Expected Output:**
```
P-value: 0.00974
Observed count: 17
Expected (null): 8.24
Cohen's d: 4.19
Bayes Factor: 21.6
âœ“ Pattern validated
```

---

### Example 2: Custom Analysis Pipeline

```python
from ancient_text_dsh import AnalysisConfig, AncientTextAnalysisPipeline

# Configure analysis
config = AnalysisConfig(
    data_dir='custom/path',
    output_dir='custom/output',
    random_seed=123,
    n_permutations=20000,
    n_bayesian_draws=5000,
    enable_bayesian=True,
    significance_level=0.01,
    fdr_level=0.05
)

# Run pipeline
pipeline = AncientTextAnalysisPipeline(config)
results = pipeline.run_complete_analysis()

# Access results
print(f"Validated patterns: {len(results['validated_patterns'])}")
print(f"Mean Bayes Factor: {results['summary']['mean_bayes_factor']:.2f}")
print(f"FDR-adjusted significance: {results['summary']['fdr_threshold']:.4f}")
```

---

### Example 3: Multi-Cultural Gematria

```python
from ancient_text_dsh import compute_gematria, CulturalSystem

# Hebrew (standard)
hebrew_value = compute_gematria('×‘×¨××©×™×ª', CulturalSystem.HEBREW_STANDARD)
print(f"Hebrew (standard): {hebrew_value}")  # 913

# Hebrew (Atbash)
atbash_value = compute_gematria('×‘×¨××©×™×ª', CulturalSystem.HEBREW_ATBASH)
print(f"Hebrew (Atbash): {atbash_value}")  # 1235

# Greek Isopsephy
greek_value = compute_gematria('Î»ÏŒÎ³Î¿Ï‚', CulturalSystem.GREEK_ISOPSEPHY)
print(f"Greek: {greek_value}")  # 373

# Arabic Abjad
arabic_value = compute_gematria('Ø¨Ø³Ù…', CulturalSystem.ARABIC_ABJAD)
print(f"Arabic: {arabic_value}")  # 102
```

---

### Example 4: Batch Processing

```python
from pathlib import Path
from ancient_text_dsh import AnalysisConfig, AncientTextAnalysisPipeline

# Process entire corpus
corpus_files = Path('corpus').glob('*.txt')

for file in corpus_files:
    print(f"Analyzing {file.name}...")
    
    config = AnalysisConfig(
        data_dir=file.parent,
        output_dir=Path('results') / file.stem,
        n_permutations=50000
    )
    
    pipeline = AncientTextAnalysisPipeline(config)
    results = pipeline.run_complete_analysis()
