[readme_en_dsh.md(1).md](https://github.com/user-attachments/files/23503661/readme_en_dsh.md.1.md)
# ğŸ“Š Computational Analysis of Numerical Patterns in Ancient Texts

[![DOI]((https://zenodo.org/badge/DOI/10.5281/zenodo.17591679.svg)](https://doi.org/10.5281/zenodo.17591679))]
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Code Coverage](https://img.shields.io/badge/coverage-87%25-brightgreen.svg)](tests/)
[![DSH Submission](https://img.shields.io/badge/status-under%20review-orange.svg)](https://academic.oup.com/dsh)

A rigorous, reproducible computational framework for detecting and validating numerical patterns in ancient texts using multiple cultural systems (Hebrew, Greek, Arabic) with comprehensive statistical validation, Bayesian inference, and ethical considerations.

**Publication Status**: Submitted to *Digital Scholarship in the Humanities* (DSH)  
**Author**: Ahmed Benseddik  
**Version**: 4.5-DSH  
**Date**: November 2025

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Case Study: Genesis](#-case-study-genesis)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Repository Structure](#-repository-structure)
- [Methodology](#-methodology)
- [Usage Examples](#-usage-examples)
- [Analysis Pipeline](#-analysis-pipeline)
- [Interpreting Results](#-interpreting-results)
- [Testing](#-testing)
- [Documentation](#-documentation)
- [Reproducibility](#-reproducibility)
- [Citation](#-citation)
- [Contributing](#-contributing)
- [License](#-license)
- [Contact](#-contact)

---

## ğŸ¯ Overview

This repository contains the complete implementation of a three-phase computational framework for detecting numerical patterns in ancient texts, with comprehensive case studies of Genesis (Sefer Bereshit) and support for multiple cultural numerical systems. The framework is designed for digital humanities scholarship with rigorous methodological standards.

### Framework Architecture

```mermaid
graph TD
    A[Ancient Text] --> B[Phase 1: Discovery]
    B --> C[Frequency Analysis]
    B --> D[Gematria Calculation]
    B --> E[Positional Clustering]
    
    C --> F[Phase 2: Statistical Validation]
    D --> F
    E --> F
    
    F --> G[Permutation Tests]
    F --> H[Bayesian Analysis]
    F --> I[Bootstrap CI]
    F --> J[FDR Correction]
    
    G --> K[Phase 3: Expert Consensus]
    H --> K
    I --> K
    J --> K
    
    K --> L[Diachronic Validation]
    L --> M[Validated Patterns]
    
    style A fill:#e1f5ff
    style M fill:#d4edda
    style F fill:#fff3cd
    style K fill:#f8d7da
```

The framework combines:

1. **Unsupervised Discovery** â€” Pattern detection via frequency analysis, gematria, and permutation scans
2. **Statistical Validation** â€” Multiple tests (permutation, Bayesian, bootstrap) with FDR corrections
3. **Expert Consensus** â€” Structured Delphi protocol with interdisciplinary panel

---

## âœ¨ Key Features

### ğŸ”¬ Methodological Innovation

âœ… **First integrated framework** combining frequentist, Bayesian, and qualitative validation  
âœ… **Rigorous anti-p-hacking protocol** â€” Pre-registered markers and discovery-validation split  
âœ… **Multiple cultural systems** â€” Hebrew gematria (standard, Atbash, Albam), Greek isopsephy, Arabic abjad  
âœ… **Formal mathematical proofs** â€” 7 theorems with computational verification  
âœ… **Diachronic validation** â€” Manuscript stability across 1100 years (Qumran â†’ Leningrad)  
âœ… **Complete reproducibility** â€” All code, data, and parameters publicly available

### ğŸ“Š Statistical Methods

#### Frequentist Validation
- **Permutation tests**: 10,000-50,000 iterations with exact p-values
- **Binomial tests**: Exact confidence intervals (Wilson score method)
- **Multiple testing corrections**: Bonferroni, Å idÃ¡k, Benjamini-Hochberg FDR
- **Effect sizes**: Cohen's h, Cohen's d, standardized differences
- **Bootstrap CI**: Percentile and BCa methods (10,000 resamples)
- **Power analysis**: Sample size adequacy assessment (target power â‰¥ 0.80)

#### Bayesian Validation
- **Hierarchical models**: Beta-Binomial conjugate priors
- **MCMC sampling**: PyMC with 4 chains, 5000+ draws, Gelman-Rubin diagnostics
- **Convergence diagnostics**: RÌ‚, effective sample size, trace plots
- **Model comparison**: WAIC, LOO-CV, Bayes Factors (BF)
- **Posterior predictive checks**: Distribution validation
- **HDI intervals**: Highest Density Intervals (95% credible intervals)

#### Non-Parametric Validation
- **Distribution tests**: Shapiro-Wilk, Anderson-Darling, Kolmogorov-Smirnov
- **Q-Q plots**: Quantile-quantile comparisons
- **Permutation-based CI**: Distribution-free inference

### ğŸ”„ Reproducibility Guarantees

- âœ… **Complete environment capture**: Python version, dependencies, system info
- âœ… **Git commit tracking**: Version control integration with tagged releases
- âœ… **Deterministic seeds**: All random processes reproducible (seed=42)
- âœ… **Comprehensive logging**: File + console outputs with timestamps
- âœ… **Metadata tracking**: Every analysis run documented with provenance
- âœ… **Pre-registration**: OSF registry for markers and parameters (locked record)
- âœ… **Code verification**: Independent R implementation validates Python results

### ğŸŒ Multi-Cultural Numerical Systems

- **Hebrew Gematria**:
  - Standard (Mispar Hechrachi): Traditional Hebrew letter values
  - Atbash (letter reversal): ×â†”×ª, ×‘â†”×©, etc.
  - Albam (letter substitution): ×â†”×œ, ×‘â†”×, etc.
- **Greek Isopsephy**: Classical Greek numerical values (Î±=1, Î²=2, ..., Ï‰=800)
- **Arabic Abjad**: Traditional Arabic numerals (Ø£=1, Ø¨=2, Ø¬=3, etc.)
- **Cross-cultural correlation**: Statistical comparison across systems

### ğŸ“ˆ Visualizations and Reporting

- ğŸ“Š **Publication-quality figures**: 300 DPI, vector formats (SVG, PDF)
- ğŸ“ˆ **Distribution plots**: Histograms with density curves, Q-Q plots
- ğŸ¨ **Bayesian diagnostics**: Trace plots, posterior distributions, forest plots
- ğŸ” **Sensitivity analysis**: Robustness visualizations across parameter space
- ğŸŒ **Cross-cultural heatmaps**: Correlation matrices for multi-system analysis
- ğŸ“‰ **Effect size plots**: Forest plots with confidence intervals

### ğŸ”¬ Ethical Considerations

- ğŸ”¬ **Methodological transparency**: All assumptions documented and justified
- ğŸŒ **Cultural sensitivity**: Guidelines for respectful interpretation of religious texts
- âš ï¸ **Interpretation caveats**: Limitations clearly stated in all outputs
- ğŸ“ **Acknowledgment of uncertainty**: Probabilistic statements only, no deterministic claims
- ğŸ¤ **Community engagement**: Open to scholarly feedback and collaborative development

---

## ğŸ“– Case Study: Genesis

### Validated Patterns

| Pattern | Hebrew | Value/Count | p-value | Bayes Factor | Expert Score | Stability |
|---------|--------|-------------|---------|--------------|--------------|-----------|
| **Toledot** | ×ª×•×œ×“×•×ª | 846 (gematria) | 0.007 | 18.7 | 8.2/10 | 96.7% |
| **Ha-Tebah** | ×”×ª×‘×” | 17 occurrences | 0.010 | 21.6 | 8.3/10 | 98.0% |
| **Sum 1260** | â€” | 3 instances | 0.012 | 14.3 | 7.9/10 | 100% |
| **Sum 1290** | â€” | 2 instances | 0.019 | 12.4 | 8.1/10 | 100% |
| **Sum 1335** | â€” | 2 instances | 0.015 | 14.9 | 7.5/10 | 100% |

**All patterns significant after FDR correction (q < 0.05)**

### Detailed Results

#### 1. ×ª×•×œ×“×•×ª (Toledot, "Generations")
- **Gematria value**: 846 marks 10 structural divisions in Genesis
- **Validation**: BF=18.7 (strong evidence), p<0.01, expert consensus 8.2/10
- **Interpretation**: Well-known structural marker in biblical scholarship; gematria alignment reinforces architectural significance
- **Biblical context**: Toledot formulas divide Genesis into literary units (Gen 2:4, 5:1, 6:9, 10:1, etc.)

#### 2. ×”×ª×‘×” (Ha-Tebah, "The Ark")
- **Occurrences**: 17 times in Genesis
- **Clustering**: At narrative markers (p<0.01, Cohen's d=4.19)
- **Context**: Specific to Noah narrative (Genesis 6-9)
- **Robustness**: Pattern remains significant when analyzed within Noah narrative alone (p=0.023)

#### 3. Intertextual Sums (1260, 1290, 1335)
- **Correlations**: With prophetic chronologies (Daniel 12, Revelation 11-12)
- **Validation**: All BF > 12, expert consensus â‰¥ 7.5
- **Manuscript stability**: 100% across witnesses (Aleppo, Leningrad)
- **Significance**: Potential numerical intertextuality across biblical corpus

### Robustness Testing

âœ… **Alternative markers**: Patterns robust across 3 marker definitions (p â‰¤ 0.02 in all)  
âœ… **Subsampling**: Ha-Tebah specific to Noah narrative (as expected; p=0.18 when excluded)  
âœ… **Random seed variation**: P-values stable within Â±0.005 across 10 seeds  
âœ… **Manuscript variations**: 91-100% stability across Qumran, Aleppo, Leningrad codices

---

## ğŸš€ Installation

### Prerequisites

- **Python** 3.9 or higher
- **Git** (for cloning repository)
- **(Optional)** LaTeX distribution for compiling mathematical proofs

### Standard Installation

```bash
# Clone the repository
git clone https://github.com/benseddikahmed-sudo/Ancient-Text-Numerical-Analysis-v-0.4.git
cd Ancient-Text-Numerical-Analysis-v-0.4

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Required Packages

```txt
numpy>=1.24.0           # Numerical computing
scipy>=1.10.0           # Scientific computing
pandas>=2.0.0           # Data manipulation
matplotlib>=3.7.0       # Plotting
seaborn>=0.12.0         # Statistical visualization
statsmodels>=0.14.0     # Statistical models
jupyter>=1.0.0          # Interactive notebooks
pytest>=7.0.0           # Testing framework
pymc>=5.0.0             # Bayesian inference (optional)
arviz>=0.15.0           # Bayesian diagnostics (optional)
numba>=0.57.0           # JIT compilation (optional)
```

### Minimal Installation (without Bayesian)

If you only need frequentist methods:

```bash
pip install numpy scipy pandas matplotlib seaborn statsmodels
```

### Verify Installation

```bash
# Run test suite
python -m pytest tests/ -v

# Run theorem demonstrations
python src/theorem_demonstrations.py

# Check environment
python -c "import ancient_text_dsh; print(ancient_text_dsh.__version__)"
```

**All tests should pass âœ…**

---

## ğŸƒ Quick Start

### Full Analysis

```bash
# Complete analysis with all features
python ancient_text_dsh.py --data-dir ./data/genesis --output-dir ./results

# Fast analysis (no Bayesian, fewer permutations)
python ancient_text_dsh.py --no-bayesian --n-permutations 10000

# High-quality analysis (publication-ready)
python ancient_text_dsh.py --n-permutations 50000 --n-bayesian-draws 5000 --dpi 300
```

### Python Example

```python
from src.permutation_tests import permutation_test
from src.bayesian_analysis import bayes_factor_binomial
import json

# Load configuration
with open('data/analysis_config.json', 'r') as f:
    config = json.load(f)

# Load markers
with open('data/structural_markers.json', 'r') as f:
    markers = json.load(f)

# Run permutation test for Ha-Tebah
result = permutation_test(
    corpus='data/genesis_leningrad.txt',
    target_term='×”×ª×‘×”',
    markers=markers['chapter_boundaries'],
    n_iterations=50000,
    seed=42
)

print(f"P-value: {result['p_value']:.5f}")
print(f"Observed count: {result['observed_count']}")
print(f"Expected (null): {result['null_mean']:.2f}")
print(f"Cohen's d: {result['cohens_d']:.2f}")

# Bayes Factor
bf = bayes_factor_binomial(
    observed_count=17,
    n_markers=43,
    corpus_length=20614,
    total_occurrences=17,
    alpha_prior=5.0,
    beta_prior=2.0
)

print(f"Bayes Factor: {bf:.1f}")
```

**Expected Output**:
```
P-value: 0.00974
Observed count: 17
Expected (null): 8.24
Cohen's d: 4.19
Bayes Factor: 21.6
âœ“ Pattern validated
```

### Custom Analysis

```python
from ancient_text_dsh import AnalysisConfig, AncientTextAnalysisPipeline

# Configure analysis
config = AnalysisConfig(
    data_dir='custom/path',
    output_dir='custom/output',
    random_seed=123,
    n_permutations=20000,
    n_bayesian_draws=5000,
    enable_bayesian=True,
    significance_level=0.01,
    fdr_level=0.05
)

# Run pipeline
pipeline = AncientTextAnalysisPipeline(config)
results = pipeline.run_complete_analysis()

# Access results
print(f"Validated patterns: {len(results['validated_patterns'])}")
print(f"Mean Bayes Factor: {results['summary']['mean_bayes_factor']:.2f}")
print(f"FDR-adjusted significance: {results['summary']['fdr_threshold']:.4f}")
```

### Interactive Notebooks

```bash
jupyter notebook notebooks/
```

Start with:
- `01_exploratory_analysis.ipynb` â€” Data exploration and visualization
- `02_permutation_tests.ipynb` â€” Statistical testing walkthrough
- `03_bayesian_validation.ipynb` â€” Bayesian inference tutorial
- `04_diachronic_checks.ipynb` â€” Manuscript comparison

---

## ğŸ“ Repository Structure

```
Ancient-Text-Numerical-Analysis-v-0.4/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ CHANGELOG.md                       # Version history
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.py                           # Package installation
â”‚
â”œâ”€â”€ data/                              # Source texts and annotations
â”‚   â”œâ”€â”€ genesis_leningrad.txt          # Westminster Leningrad Codex (Genesis)
â”‚   â”œâ”€â”€ structural_markers.json        # Pre-registered markers (43 total)
â”‚   â”œâ”€â”€ gematria_map.csv              # Hebrew letter â†’ numeric values
â”‚   â”œâ”€â”€ key_patterns.json              # 5 validated patterns with stats
â”‚   â”œâ”€â”€ analysis_config.json           # Pre-registered parameters
â”‚   â””â”€â”€ cultural_systems/              # Greek, Arabic mappings
â”‚       â”œâ”€â”€ greek_isopsephy.json
â”‚       â””â”€â”€ arabic_abjad.json
â”‚
â”œâ”€â”€ src/                               # Core analysis modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ancient_text_dsh.py            # Main analysis script
â”‚   â”œâ”€â”€ permutation_tests.py           # Permutation test implementation
â”‚   â”œâ”€â”€ bayesian_analysis.py           # Bayes Factor calculations
â”‚   â”œâ”€â”€ gematria_calculator.py         # Multi-cultural gematria
â”‚   â”œâ”€â”€ diachronic_validation.py       # Manuscript comparison
â”‚   â”œâ”€â”€ expert_panel_analysis.py       # Delphi protocol scoring
â”‚   â”œâ”€â”€ fdr_correction.py              # Benjamini-Hochberg FDR
â”‚   â”œâ”€â”€ visualization_tools.py         # Plotting functions
â”‚   â””â”€â”€ theorem_demonstrations.py      # Mathematical proofs verification
â”‚
â”œâ”€â”€ notebooks/                         # Interactive analysis
â”‚   â”œâ”€â”€ 01_exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ 02_permutation_tests.ipynb
â”‚   â”œâ”€â”€ 03_bayesian_validation.ipynb
â”‚   â”œâ”€â”€ 04_diachronic_checks.ipynb
â”‚   â”œâ”€â”€ 05_expert_panel_analysis.ipynb
â”‚   â””â”€â”€ 06_sensitivity_analyses.ipynb
â”‚
â”œâ”€â”€ results/                           # Analysis outputs
â”‚   â”œâ”€â”€ permutation_outputs.csv        # P-values for all patterns
â”‚   â”œâ”€â”€ bayes_factors.csv              # BF calculations
â”‚   â”œâ”€â”€ expert_scores.csv              # Delphi panel results
â”‚   â”œâ”€â”€ diachronic_stability.csv       # Manuscript preservation
â”‚   â”œâ”€â”€ theorem_verification_results.json
â”‚   â””â”€â”€ figures/                       # Publication-ready plots
â”‚       â”œâ”€â”€ theorem1_type1_control.png
â”‚       â”œâ”€â”€ theorem2_bf_consistency.png
â”‚       â”œâ”€â”€ theorem3_fdr_control.png
â”‚       â”œâ”€â”€ gematria_distribution.png
â”‚       â”œâ”€â”€ multiples_analysis.png
â”‚       â””â”€â”€ cross_cultural_heatmap.png
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ METHODOLOGY.md                 # Detailed methods
â”‚   â”œâ”€â”€ mathematical_proofs.pdf        # Complete proofs (25 pages)
â”‚   â”œâ”€â”€ mathematical_proofs.tex        # LaTeX source
â”‚   â”œâ”€â”€ proofs_summary.pdf             # 5-page summary
â”‚   â”œâ”€â”€ references.bib                 # BibTeX bibliography (40+ refs)
â”‚   â”œâ”€â”€ technical_slide.html           # Permutation visualization
â”‚   â”œâ”€â”€ infographic.html               # Framework visual summary
â”‚   â””â”€â”€ appendix_A_methodology.md      # Complete technical appendix
â”‚
â”œâ”€â”€ tests/                             # Unit and integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_permutation.py
â”‚   â”œâ”€â”€ test_bayesian.py
â”‚   â”œâ”€â”€ test_gematria.py
â”‚   â”œâ”€â”€ test_fdr.py
â”‚   â”œâ”€â”€ test_statistics.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â””â”€â”€ supplementary/                     # Additional materials
    â”œâ”€â”€ presentation_beamer.pdf        # Conference slides
    â”œâ”€â”€ poster_DSH2025.pdf             # Conference poster
    â””â”€â”€ media/                         # Presentation figures
```

---

## ğŸ”¬ Methodology

### Phase 1: Unsupervised Discovery

Unsupervised detection of pattern candidates:

- **Input**: Text corpus T, pre-registered markers M
- **Output**: Candidate patterns exceeding k=2 standard deviations
- **Methods**:
  - Frequency analysis (lexical distribution)
  - Gematria calculation (multiple cultural systems)
  - Co-occurrence detection (term proximity analysis)
  - Positional clustering (structural marker association)

**Critical**: Discovery phase uses no hypothesis testing to avoid data mining. All candidates subjected to independent validation.

### Phase 2: Multi-Method Statistical Validation

Statistical validation with multiple independent methods:

- **Permutation tests**: 10,000-50,000 iterations, exact p-values
  - Null model: Random lexical permutation preserving frequencies
  - One-tailed test: P(X â‰¥ observed | Hâ‚€)
  - Effect size: Cohen's d with 95% bootstrap CI

- **Bayesian analysis**: Bayes Factors with Beta priors (BF > 10 threshold)
  - Model comparison: Hâ‚€ (random) vs. Hâ‚ (structured)
  - Hierarchical Beta-Binomial models
  - Prior sensitivity analysis

- **Bootstrap CI**: 10,000 resamples, 95% confidence intervals
  - BCa (bias-corrected and accelerated) method
  - Percentile method for comparison

- **FDR correction**: Benjamini-Hochberg at q=0.05
  - Controls expected proportion of false discoveries
  - More powerful than Bonferroni for multiple hypotheses

- **Effect sizes**: Cohen's d, h with interpretation guidelines
  - Small (0.2), medium (0.5), large (0.8) effects
  - Standardized for cross-pattern comparison

- **Power analysis**: Sample size adequacy (target power â‰¥ 0.80)
  - Post-hoc power calculation
  - Ensures sufficient sensitivity to detect effects

### Phase 3: Structured Expert Consensus

Expert consensus via modified Delphi protocol:

- **Panel**: 12 experts (4 philologists, 3 statisticians, 3 historians, 2 textual critics)
- **Protocol**: Modified Delphi with 3 rounds
  - **Round 1**: Blind assessment (no statistical results)
  - **Round 2**: Re-evaluation with statistical disclosure
  - **Round 3**: Consensus discussion with facilitation

- **Scoring**: 0-10 scale across 4 criteria
  - Historical plausibility (0-3 points)
  - Textual coherence (0-3 points)
  - Manuscript stability (0-2 points)
  - Statistical strength (0-2 points)

- **Threshold**: Mean â‰¥ 7.0 with SD â‰¤ 1.5

### Combined Validation Criteria

A pattern is validated if and only if **ALL** criteria are met:

âœ… Permutation p-value < 0.01  
âœ… Bayes Factor > 10 (strong evidence)  
âœ… Expert consensus â‰¥ 7.0  
âœ… Diachronic stability â‰¥ 90%

**Mathematical Formulation**:

**Theorem (Combined Type-I Error Control)**: Under the global null hypothesis, the framework controls family-wise error rate at Î± â‰¤ 0.05.

**Proof**: See [`docs/mathematical_proofs.pdf`](docs/mathematical_proofs.pdf) (Theorem 4, page 12).

---

## ğŸ’» Usage Examples

### Multi-Cultural Gematria Calculation

```python
from ancient_text_dsh import compute_gematria, CulturalSystem

# Hebrew (standard)
hebrew_value = compute_gematria('×‘×¨××©×™×ª', CulturalSystem.HEBREW_STANDARD)
print(f"Hebrew (standard): {hebrew_value}")  # 913

# Hebrew (Atbash)
atbash_value = compute_gematria('×‘×¨××©×™×ª', CulturalSystem.HEBREW_ATBASH)
print(f"Hebrew (Atbash): {atbash_value}")    # 1235

# Greek Isopsephy
greek_value = compute_gematria('Î»ÏŒÎ³Î¿Ï‚', CulturalSystem.GREEK_ISOPSEPHY)
print(f"Greek: {greek_value}")  # 373

# Arabic Abjad
arabic_value = compute_gematria('Ø¨Ø³Ù…', CulturalSystem.ARABIC_ABJAD)
print(f"Arabic: {arabic_value}")  # 102
```

### Batch Processing

```python
from pathlib import Path
from ancient_text_dsh import AnalysisConfig, AncientTextAnalysisPipeline

# Process entire corpus
corpus_files = Path('corpus').glob('*.txt')

for file in corpus_files:
    print(f"Analyzing {file.name}...")
    
    config = AnalysisConfig(
        data_dir=file.parent,
        output_dir=Path('results') / file.stem,
        n_permutations=50000
    )
    
    pipeline = AncientTextAnalysisPipeline(config)
    results = pipeline.run_complete_analysis()
    
    print(f"  Validated: {len(results['validated_patterns'])} patterns")
```

### Custom Pattern Analysis

```python
from src.permutation_tests import permutation_test
import numpy as np

# Your custom data
observed_frequencies = [17, 12, 23, 9, 15]
expected_baseline = 0.15  # Expected proportion

# Run permutation test
result = permutation_test(
    observed=observed_frequencies,
    baseline=expected_baseline,
    n_iterations=50000,
    seed=42
)

print(f"P-value: {result['p_value']:.5f}")
print(f"Cohen's d: {result['cohens_d']:.2f}")
print(f"95% CI: [{result['ci_lower']:.2f}, {result['ci_upper']:.2f}]")
```

### Bayesian Model Comparison

```python
from src.bayesian_analysis import bayesian_model_comparison

# Compare null vs. enrichment models
comparison = bayesian_model_comparison(
    data=observed_counts,
    model_null='binomial',
    model_alternative='beta_binomial',
    n_samples=5000
)

print(f"WAIC difference: {comparison['delta_waic']:.2f}")
print(f"Evidence for alternative: {comparison['interpretation']}")
print(f"Bayes Factor: {comparison['bayes_factor']:.1f}")
```

---

## ğŸ“Š Analysis Pipeline

```
1. Data Preprocessing
   â”œâ”€â”€ Validate encoding (UTF-8)
   â”œâ”€â”€ Normalize text (final letter forms)
   â”œâ”€â”€ Extract segments (windows)
   â””â”€â”€ Compute numerical values

2. Gematria Analysis
   â”œâ”€â”€ Statistical summaries
   â”œâ”€â”€ Distribution testing
   â”œâ”€â”€ Cross-cultural comparison
   â””â”€â”€ Visualization

3. Frequentist Validation
   â”œâ”€â”€ Multiples enrichment (7, 12, 26, 30, 60)
   â”œâ”€â”€ Binomial tests
   â”œâ”€â”€ Permutation tests (10k-50k iter)
   â”œâ”€â”€ FDR correction
   â””â”€â”€ Effect sizes + CI

4. Bayesian Analysis (optional)
   â”œâ”€â”€ Hierarchical modeling
   â”œâ”€â”€ MCMC sampling (4 chains)
   â”œâ”€â”€ Convergence diagnostics
   â”œâ”€â”€ Model comparison (WAIC/LOO)
   â””â”€â”€ Posterior predictive checks

5. Sensitivity Analysis
   â”œâ”€â”€ Window size variations
   â”œâ”€â”€ Sampling strategies
   â”œâ”€â”€ Parameter robustness
   â””â”€â”€ Bootstrap stability

6. Expert Validation
   â”œâ”€â”€ Delphi Round 1 (blind)
   â”œâ”€â”€ Delphi Round 2 (with stats)
   â”œâ”€â”€ Delphi Round 3 (consensus)
   â””â”€â”€ Final scoring

7. Diachronic Validation
   â”œâ”€â”€ Manuscript comparison
   â”œâ”€â”€ Stability calculation
   â””â”€â”€ Transmission analysis

8. Report Generation
   â”œâ”€â”€ JSON results
   â”œâ”€â”€ Markdown report
   â”œâ”€â”€ Publication figures
   â””â”€â”€ Summary tables
```

---

## ğŸ“ˆ Interpreting Results

### Significance Thresholds

| Criterion | Threshold | Interpretation |
|-----------|-----------|----------------|
| **P-value** | < 0.01 | Highly significant (after FDR correction) |
| | 0.01-0.05 | Significant |
| | > 0.05 | Not significant |
| **Bayes Factor** | > 100 | Decisive evidence for Hâ‚ |
| | 30-100 | Very strong evidence |
| | 10-30 | Strong evidence |
| | 3-10 | Moderate evidence |
| | 1-3 | Weak evidence |
| | < 1 | Evidence for Hâ‚€ |
| **Effect Size (d)** | > 2.0 | Very large effect |
| | 0.8-2.0 | Large effect |
| | 0.5-0.8 | Medium effect |
| | 0.2-0.5 | Small effect |
| | < 0.2 | Negligible effect |
| **Expert Score** | â‰¥ 7.0 | Pattern probably meaningful |
| | 4.0-7.0 | Uncertain, needs more evidence |
| | < 4.0 | Probably spurious |
| **Stability** | â‰¥ 90% | Robust across manuscripts |
| | 70-90% | Moderate stability |
| | < 70% | Questionable transmission |

### WAIC/LOO Interpretation

- **Î”WAIC < 2**: Models similar, no clear preference
- **2 < Î”WAIC < 6**: Moderate evidence for better model
- **Î”WAIC > 6**: Strong evidence for better model

### Sensitivity and Robustness

- **CV < 0.3**: Robust results, conclusions reliable
- **0.3 < CV < 0.5**: Moderate sensitivity, interpret with caution
- **CV > 0.5**: High sensitivity, results unstable

### Power Analysis

- **Power > 0.8**: Adequate sample size for detecting effect
- **0.6 < Power < 0.8**: Moderate power, consider larger sample
- **Power < 0.6**: Underpowered, high risk of Type-II error

### Combined Validation

For full validation, a pattern should demonstrate:

âœ… Statistical significance (p < 0.01, BF > 10)  
âœ… Large effect size (d > 0.8)  
âœ… Expert consensus (score â‰¥ 7.0)  
âœ… Manuscript stability (â‰¥ 90%)  
âœ… Robustness to variations (CV < 0.5)

---

## ğŸ§ª Testing

### Complete Test Suite

```bash
# All tests with coverage
pytest tests/ -v --cov=ancient_text_dsh --cov-report=html

# Quick smoke tests
pytest tests/ -x -v

# Specific test categories
pytest tests/test_gematria.py -v        # Gematria calculations
pytest tests/test_statistics.py -v      # Statistical methods
pytest tests/test_pipeline.py -v        # Integration tests
pytest tests/test_bayesian.py -v        # Bayesian inference
```

### Coverage Report

```bash
pytest tests/ --cov=ancient_text_dsh --cov-report=term-missing
```

**Coverage target**: > 85%

### Theorem Verification

```bash
python src/theorem_demonstrations.py
```

**Expected Output**:
```
======================================================================
THEOREM 1: Type-I Error Control
